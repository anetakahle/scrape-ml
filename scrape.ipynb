{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "sqlite3.sqlite_version\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "database = sqlite3.connect('outdb.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df_benchmarks = pd.read_sql_query(\"SELECT * from Benchmarks\", database)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "    id                           name   \n0    1                       Vimeo90K  \\\n1    2  MSU Video Frame Interpolation   \n2    3                         UCF101   \n3    4                     X4K1000FPS   \n4    5                     Middlebury   \n5    6            Vid4 - 4x upscaling   \n6    7                SNU-FILM (easy)   \n7    8              SNU-FILM (medium)   \n8    9                SNU-FILM (hard)   \n9   10             SNU-FILM (extreme)   \n10  11                  X4K1000FPS-2K   \n11  12                        Xiph-2K   \n12  13                        Xiph-4k   \n13  14                          DAVIS   \n14  15                         VFITex   \n15  16                        ATD-12K   \n16  17                 Xiph-4K (Crop)   \n17  18           Nvidia Dynamic Scene   \n18  19                        Xiph 4k   \n\n                                                  url  \n0         /sota/video-frame-interpolation-on-vimeo90k  \n1   /sota/video-frame-interpolation-on-msu-video-f...  \n2         /sota/video-frame-interpolation-on-ucf101-1  \n3       /sota/video-frame-interpolation-on-x4k1000fps  \n4       /sota/video-frame-interpolation-on-middlebury  \n5          /sota/video-frame-interpolation-on-vid4-4x  \n6    /sota/video-frame-interpolation-on-snu-film-easy  \n7   /sota/video-frame-interpolation-on-snu-film-me...  \n8    /sota/video-frame-interpolation-on-snu-film-hard  \n9   /sota/video-frame-interpolation-on-snu-film-ex...  \n10   /sota/video-frame-interpolation-on-x4k1000fps-2k  \n11         /sota/video-frame-interpolation-on-xiph-2k  \n12       /sota/video-frame-interpolation-on-xiph-4k-1  \n13           /sota/video-frame-interpolation-on-davis  \n14          /sota/video-frame-interpolation-on-vfitex  \n15         /sota/video-frame-interpolation-on-atd-12k  \n16    /sota/video-frame-interpolation-on-xiph-4k-crop  \n17  /sota/video-frame-interpolation-on-nvidia-dynamic  \n18         /sota/video-frame-interpolation-on-xiph-4k  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Vimeo90K</td>\n      <td>/sota/video-frame-interpolation-on-vimeo90k</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>MSU Video Frame Interpolation</td>\n      <td>/sota/video-frame-interpolation-on-msu-video-f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>UCF101</td>\n      <td>/sota/video-frame-interpolation-on-ucf101-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>X4K1000FPS</td>\n      <td>/sota/video-frame-interpolation-on-x4k1000fps</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Middlebury</td>\n      <td>/sota/video-frame-interpolation-on-middlebury</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>Vid4 - 4x upscaling</td>\n      <td>/sota/video-frame-interpolation-on-vid4-4x</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>SNU-FILM (easy)</td>\n      <td>/sota/video-frame-interpolation-on-snu-film-easy</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>SNU-FILM (medium)</td>\n      <td>/sota/video-frame-interpolation-on-snu-film-me...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>SNU-FILM (hard)</td>\n      <td>/sota/video-frame-interpolation-on-snu-film-hard</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>SNU-FILM (extreme)</td>\n      <td>/sota/video-frame-interpolation-on-snu-film-ex...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>X4K1000FPS-2K</td>\n      <td>/sota/video-frame-interpolation-on-x4k1000fps-2k</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>Xiph-2K</td>\n      <td>/sota/video-frame-interpolation-on-xiph-2k</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>Xiph-4k</td>\n      <td>/sota/video-frame-interpolation-on-xiph-4k-1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>DAVIS</td>\n      <td>/sota/video-frame-interpolation-on-davis</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>VFITex</td>\n      <td>/sota/video-frame-interpolation-on-vfitex</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>ATD-12K</td>\n      <td>/sota/video-frame-interpolation-on-atd-12k</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>Xiph-4K (Crop)</td>\n      <td>/sota/video-frame-interpolation-on-xiph-4k-crop</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>Nvidia Dynamic Scene</td>\n      <td>/sota/video-frame-interpolation-on-nvidia-dynamic</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>Xiph 4k</td>\n      <td>/sota/video-frame-interpolation-on-xiph-4k</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_benchmarks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "      id  modelId  benchmarkId    psnr    ssim  lpips   \n0      1        1            1  36.760  0.9800    NaN  \\\n1      2        2            1  36.640  0.9819    NaN   \n2      3        3            1  36.420  0.9815    NaN   \n3      4        4            1  36.200  0.9808    NaN   \n4      5        5            1  36.190  0.9810    NaN   \n..   ...      ...          ...     ...     ...    ...   \n117  118       33           15  29.175     NaN    NaN   \n118  119       11           16  29.030  0.9590    NaN   \n119  120       11           17  33.930  0.9450    NaN   \n120  121       10           18  36.240  0.9839    NaN   \n121  122       10           19  30.940  0.9389    NaN   \n\n                                            detailHref  \n0    /paper/exploring-motion-ambiguity-and-alignmen...  \n1    /paper/extracting-motion-and-appearance-via-inter  \n2    /paper/a-unified-pyramid-recurrent-network-for...  \n3    /paper/ifrnet-intermediate-feature-refine-network  \n4    /paper/enhanced-bi-directional-motion-estimati...  \n..                                                 ...  \n117  /paper/spatio-temporal-multi-flow-network-for-...  \n118  /paper/many-to-many-splatting-for-efficient-video  \n119  /paper/many-to-many-splatting-for-efficient-video  \n120  /paper/learning-cross-video-neural-representat...  \n121  /paper/learning-cross-video-neural-representat...  \n\n[122 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>modelId</th>\n      <th>benchmarkId</th>\n      <th>psnr</th>\n      <th>ssim</th>\n      <th>lpips</th>\n      <th>detailHref</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>36.760</td>\n      <td>0.9800</td>\n      <td>NaN</td>\n      <td>/paper/exploring-motion-ambiguity-and-alignmen...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>36.640</td>\n      <td>0.9819</td>\n      <td>NaN</td>\n      <td>/paper/extracting-motion-and-appearance-via-inter</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>36.420</td>\n      <td>0.9815</td>\n      <td>NaN</td>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>36.200</td>\n      <td>0.9808</td>\n      <td>NaN</td>\n      <td>/paper/ifrnet-intermediate-feature-refine-network</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>36.190</td>\n      <td>0.9810</td>\n      <td>NaN</td>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>118</td>\n      <td>33</td>\n      <td>15</td>\n      <td>29.175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/paper/spatio-temporal-multi-flow-network-for-...</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>119</td>\n      <td>11</td>\n      <td>16</td>\n      <td>29.030</td>\n      <td>0.9590</td>\n      <td>NaN</td>\n      <td>/paper/many-to-many-splatting-for-efficient-video</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>120</td>\n      <td>11</td>\n      <td>17</td>\n      <td>33.930</td>\n      <td>0.9450</td>\n      <td>NaN</td>\n      <td>/paper/many-to-many-splatting-for-efficient-video</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>121</td>\n      <td>10</td>\n      <td>18</td>\n      <td>36.240</td>\n      <td>0.9839</td>\n      <td>NaN</td>\n      <td>/paper/learning-cross-video-neural-representat...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>122</td>\n      <td>10</td>\n      <td>19</td>\n      <td>30.940</td>\n      <td>0.9389</td>\n      <td>NaN</td>\n      <td>/paper/learning-cross-video-neural-representat...</td>\n    </tr>\n  </tbody>\n</table>\n<p>122 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model2benchmarks = pd.read_sql_query(\"SELECT * from Model2Benchmarks\", database)\n",
    "df_model2benchmarks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1:     id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "0    1        1            1  36.76  0.9800    NaN  \\\n",
      "1    2        2            1  36.64  0.9819    NaN   \n",
      "2    3        3            1  36.42  0.9815    NaN   \n",
      "3    4        4            1  36.20  0.9808    NaN   \n",
      "4    5        5            1  36.19  0.9810    NaN   \n",
      "5    6        6            1  36.18  0.9805    NaN   \n",
      "6    7        7            1  36.10  0.9700    NaN   \n",
      "7    8        8            1  36.06  0.9700    NaN   \n",
      "8    9        9            1  35.88  0.9795    NaN   \n",
      "9   10       10            1  35.73  0.9789    NaN   \n",
      "10  11       11            1  35.40  0.9780    NaN   \n",
      "11  12       12            1  35.22  0.9643    NaN   \n",
      "12  13       13            1  35.17     NaN   0.01   \n",
      "13  14       14            1  35.07  0.9760    NaN   \n",
      "14  15       15            1  35.01  0.9764    NaN   \n",
      "15  16       16            1  34.95  0.9749    NaN   \n",
      "16  17       17            1  34.71  0.9756    NaN   \n",
      "17  18       17            1  34.71     NaN    NaN   \n",
      "18  19       18            1  34.65     NaN    NaN   \n",
      "19  20       19            1  34.40     NaN    NaN   \n",
      "20  21       20            1  33.80     NaN    NaN   \n",
      "21  22       21            1  33.73     NaN    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "0   /paper/exploring-motion-ambiguity-and-alignmen...  \n",
      "1   /paper/extracting-motion-and-appearance-via-inter  \n",
      "2   /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "3   /paper/ifrnet-intermediate-feature-refine-network  \n",
      "4   /paper/enhanced-bi-directional-motion-estimati...  \n",
      "5   /paper/asymmetric-bilateral-motion-estimation-for  \n",
      "6            /paper/softmax-splatting-for-video-frame  \n",
      "7    /paper/film-frame-interpolation-for-large-motion  \n",
      "8    /paper/neighbor-correspondence-matching-for-flow  \n",
      "9   /paper/learning-cross-video-neural-representat...  \n",
      "10  /paper/many-to-many-splatting-for-efficient-video  \n",
      "11       /paper/video-frame-interpolation-via-residue  \n",
      "12  /paper/cdfi-compression-driven-network-design-for  \n",
      "13      /paper/xvfi-extreme-video-frame-interpolation  \n",
      "14       /paper/bmbc-bilateral-motion-estimation-with  \n",
      "15   /paper/enhanced-correlation-matching-based-video  \n",
      "16       /paper/depth-aware-video-frame-interpolation  \n",
      "17       /paper/depth-aware-video-frame-interpolation  \n",
      "18                                               null  \n",
      "19       /paper/memc-net-motion-estimation-and-motion  \n",
      "20      /paper/video-frame-interpolation-via-adaptive  \n",
      "21   /paper/video-enhancement-with-task-oriented-flow  , 2:     id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "22  23       22            2  27.15  0.914  0.039  \\\n",
      "23  24       23            2  27.35  0.913  0.061   \n",
      "24  25       24            2  26.69  0.904  0.068   \n",
      "25  26        2            2  29.89  0.953  0.022   \n",
      "26  27        3            2  29.73  0.951  0.025   \n",
      "27  28       25            2  28.77  0.931  0.024   \n",
      "28  29       26            2  28.56  0.928  0.028   \n",
      "29  30       27            2  28.34  0.917  0.044   \n",
      "30  31        8            2  28.11  0.928  0.033   \n",
      "31  32       10            2  28.01  0.920  0.029   \n",
      "32  33        6            2  27.99  0.919  0.039   \n",
      "33  34       28            2  27.86  0.921  0.049   \n",
      "34  35       13            2  26.99  0.908  0.051   \n",
      "35  36       29            2  24.99  0.903  0.058   \n",
      "36  37       30            2  24.48  0.902  0.060   \n",
      "37  38       15            2  23.34  0.885  0.071   \n",
      "38  39       31            2  23.28  0.889  0.070   \n",
      "39  40       32            2  23.17  0.891  0.692   \n",
      "\n",
      "                                           detailHref  \n",
      "22  /paper/rife-real-time-intermediate-flow-estima...  \n",
      "23      /paper/xvfi-extreme-video-frame-interpolation  \n",
      "24      /paper/super-slomo-high-quality-estimation-of  \n",
      "25  /paper/extracting-motion-and-appearance-via-inter  \n",
      "26  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "27  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "28  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "29  /paper/video-frame-interpolation-with-transformer  \n",
      "30   /paper/film-frame-interpolation-for-large-motion  \n",
      "31  /paper/learning-cross-video-neural-representat...  \n",
      "32  /paper/asymmetric-bilateral-motion-estimation-for  \n",
      "33      /paper/xvfi-extreme-video-frame-interpolation  \n",
      "34  /paper/cdfi-compression-driven-network-design-for  \n",
      "35  /paper/learning-spatial-transform-for-video-frame  \n",
      "36  /paper/featureflow-robust-video-interpolation-via  \n",
      "37     /paper/bmbc-bilateral-motion-estimation-with-1  \n",
      "38  /paper/featureflow-robust-video-interpolation-via  \n",
      "39  /paper/learning-spatial-transform-for-video-frame  , 3:     id  modelId  benchmarkId    psnr    ssim  lpips   \n",
      "40  41        2            3  35.480  0.9701    NaN  \\\n",
      "41  42        3            3  35.470  0.9700    NaN   \n",
      "42  43        1            3  35.430  0.9790    NaN   \n",
      "43  44        4            3  35.420  0.9698    NaN   \n",
      "44  45        5            3  35.410  0.9700    NaN   \n",
      "45  46        7            3  35.390  0.9520    NaN   \n",
      "46  47        6            3  35.380  0.9698    NaN   \n",
      "47  48       10            3  35.360  0.9705    NaN   \n",
      "48  49        9            3  35.360  0.9695    NaN   \n",
      "49  50        8            3  35.320  0.9520    NaN   \n",
      "50  51       13            3  35.210     NaN  0.015   \n",
      "51  52       11            3  35.170  0.9700    NaN   \n",
      "52  53       15            3  35.150  0.9689    NaN   \n",
      "53  54       17            3  34.990  0.9683    NaN   \n",
      "54  55       12            3  34.930  0.9496    NaN   \n",
      "55  56       33            3  33.384     NaN    NaN   \n",
      "56  57       34            3     NaN     NaN    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "40  /paper/extracting-motion-and-appearance-via-inter  \n",
      "41  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "42  /paper/exploring-motion-ambiguity-and-alignmen...  \n",
      "43  /paper/ifrnet-intermediate-feature-refine-network  \n",
      "44  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "45           /paper/softmax-splatting-for-video-frame  \n",
      "46  /paper/asymmetric-bilateral-motion-estimation-for  \n",
      "47  /paper/learning-cross-video-neural-representat...  \n",
      "48   /paper/neighbor-correspondence-matching-for-flow  \n",
      "49   /paper/film-frame-interpolation-for-large-motion  \n",
      "50  /paper/cdfi-compression-driven-network-design-for  \n",
      "51  /paper/many-to-many-splatting-for-efficient-video  \n",
      "52       /paper/bmbc-bilateral-motion-estimation-with  \n",
      "53       /paper/depth-aware-video-frame-interpolation  \n",
      "54       /paper/video-frame-interpolation-via-residue  \n",
      "55  /paper/spatio-temporal-multi-flow-network-for-...  \n",
      "56  /paper/unsupervised-video-interpolation-using-...  , 4:     id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "57  58        9            4  31.63  0.9185    NaN  \\\n",
      "58  59        2            4  31.46     NaN    NaN   \n",
      "59  60       11            4  30.81  0.9120    NaN   \n",
      "60  61       35            4  30.68  0.9086    NaN   \n",
      "61  62       16            4  30.51  0.8719    NaN   \n",
      "62  63        6            4  30.16  0.8793    NaN   \n",
      "63  64       36            4  30.12  0.8700    NaN   \n",
      "64  65        5            4  29.46  0.9020    NaN   \n",
      "65  66       37            4  28.86  0.8580    NaN   \n",
      "66  67       38            4  27.52  0.8210    NaN   \n",
      "67  68       17            4  26.78  0.8070    NaN   \n",
      "68  69       29            4  25.81  0.7720    NaN   \n",
      "69  70       30            4  25.16  0.7830    NaN   \n",
      "70  71       31            4  24.00  0.7560    NaN   \n",
      "71  72       32            4  23.90  0.7270    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "57   /paper/neighbor-correspondence-matching-for-flow  \n",
      "58  /paper/extracting-motion-and-appearance-via-inter  \n",
      "59  /paper/many-to-many-splatting-for-efficient-video  \n",
      "60  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "61   /paper/enhanced-correlation-matching-based-video  \n",
      "62  /paper/asymmetric-bilateral-motion-estimation-for  \n",
      "63      /paper/xvfi-extreme-video-frame-interpolation  \n",
      "64  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "65      /paper/xvfi-extreme-video-frame-interpolation  \n",
      "66       /paper/depth-aware-video-frame-interpolation  \n",
      "67       /paper/depth-aware-video-frame-interpolation  \n",
      "68  /paper/learning-spatial-transform-for-video-frame  \n",
      "69  /paper/featureflow-robust-video-interpolation-via  \n",
      "70  /paper/featureflow-robust-video-interpolation-via  \n",
      "71  /paper/learning-spatial-transform-for-video-frame  , 5:     id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "72  73        4            5    NaN    NaN    NaN  \\\n",
      "73  74        7            5  38.42  0.971    NaN   \n",
      "74  75       15            5    NaN    NaN    NaN   \n",
      "75  76       18            5    NaN    NaN    NaN   \n",
      "76  77       17            5    NaN    NaN    NaN   \n",
      "77  78       39            5    NaN    NaN    NaN   \n",
      "78  79       21            5    NaN    NaN    NaN   \n",
      "79  80       20            5    NaN    NaN    NaN   \n",
      "80  81        1            5  38.83    NaN    NaN   \n",
      "81  82        8            5  37.52  0.966    NaN   \n",
      "82  83       13            5  37.14  0.966  0.007   \n",
      "\n",
      "                                           detailHref  \n",
      "72  /paper/ifrnet-intermediate-feature-refine-network  \n",
      "73           /paper/softmax-splatting-for-video-frame  \n",
      "74       /paper/bmbc-bilateral-motion-estimation-with  \n",
      "75                                               null  \n",
      "76       /paper/depth-aware-video-frame-interpolation  \n",
      "77     /paper/memc-net-motion-estimation-and-motion-1  \n",
      "78   /paper/video-enhancement-with-task-oriented-flow  \n",
      "79      /paper/video-frame-interpolation-via-adaptive  \n",
      "80  /paper/exploring-motion-ambiguity-and-alignmen...  \n",
      "81   /paper/film-frame-interpolation-for-large-motion  \n",
      "82  /paper/cdfi-compression-driven-network-design-for  , 6:     id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "83  84       40            6  27.46  0.8392    NaN  \\\n",
      "84  85       41            6  26.43  0.7994    NaN   \n",
      "85  86       42            6  26.37  0.7978    NaN   \n",
      "86  87       43            6  26.31  0.7976    NaN   \n",
      "87  88       44            6  26.29  0.7941    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "83         /paper/vrt-a-video-restoration-transformer  \n",
      "84  /paper/rstt-real-time-spatial-temporal-transfo...  \n",
      "85  /paper/rstt-real-time-spatial-temporal-transfo...  \n",
      "86  /paper/zooming-slow-mo-fast-and-accurate-one-s...  \n",
      "87  /paper/rstt-real-time-spatial-temporal-transfo...  , 7:     id  modelId  benchmarkId    psnr    ssim  lpips   \n",
      "88  89       33            7  40.775     NaN    NaN  \\\n",
      "89  90        3            7  40.440  0.9911    NaN   \n",
      "90  91        5            7  40.280  0.9910    NaN   \n",
      "91  92        2            7  39.980  0.9910    NaN   \n",
      "92  93       10            7  39.900  0.9910    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "88  /paper/spatio-temporal-multi-flow-network-for-...  \n",
      "89  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "90  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "91  /paper/extracting-motion-and-appearance-via-inter  \n",
      "92  /paper/learning-cross-video-neural-representat...  , 8:     id  modelId  benchmarkId    psnr    ssim  lpips   \n",
      "93  94       33            8  37.111     NaN    NaN  \\\n",
      "94  95        3            8  36.290  0.9801    NaN   \n",
      "95  96        2            8  36.090  0.9801    NaN   \n",
      "96  97        5            8  36.070  0.9800    NaN   \n",
      "97  98       10            8  35.940  0.9797    NaN   \n",
      "\n",
      "                                           detailHref  \n",
      "93  /paper/spatio-temporal-multi-flow-network-for-...  \n",
      "94  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "95  /paper/extracting-motion-and-appearance-via-inter  \n",
      "96  /paper/enhanced-bi-directional-motion-estimati...  \n",
      "97  /paper/learning-cross-video-neural-representat...  , 9:       id  modelId  benchmarkId    psnr    ssim  lpips   \n",
      "98    99       33            9  31.698     NaN    NaN  \\\n",
      "99   100        2            9  30.940  0.9392    NaN   \n",
      "100  101        3            9  30.860  0.9377    NaN   \n",
      "101  102       10            9  30.660  0.9373    NaN   \n",
      "102  103        5            9  30.640  0.9370    NaN   \n",
      "\n",
      "                                            detailHref  \n",
      "98   /paper/spatio-temporal-multi-flow-network-for-...  \n",
      "99   /paper/extracting-motion-and-appearance-via-inter  \n",
      "100  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "101  /paper/learning-cross-video-neural-representat...  \n",
      "102  /paper/enhanced-bi-directional-motion-estimati...  , 10:       id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "103  104       33           10  25.81     NaN    NaN  \\\n",
      "104  105        2           10  25.69  0.8661    NaN   \n",
      "105  106        3           10  25.63  0.8641    NaN   \n",
      "106  107       10           10  25.44  0.8638    NaN   \n",
      "107  108        5           10  25.40  0.8630    NaN   \n",
      "\n",
      "                                            detailHref  \n",
      "103  /paper/spatio-temporal-multi-flow-network-for-...  \n",
      "104  /paper/extracting-motion-and-appearance-via-inter  \n",
      "105  /paper/a-unified-pyramid-recurrent-network-for...  \n",
      "106  /paper/learning-cross-video-neural-representat...  \n",
      "107  /paper/enhanced-bi-directional-motion-estimati...  , 11:       id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "108  109        2           11  32.85     NaN    NaN  \\\n",
      "109  110       11           11  32.07  0.9230    NaN   \n",
      "110  111       10           11  30.05  0.8998    NaN   \n",
      "\n",
      "                                            detailHref  \n",
      "108  /paper/extracting-motion-and-appearance-via-inter  \n",
      "109  /paper/many-to-many-splatting-for-efficient-video  \n",
      "110  /paper/learning-cross-video-neural-representat...  , 12:       id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "111  112        2           12  36.90  0.945    NaN  \\\n",
      "112  113        8           12  36.66  0.951    NaN   \n",
      "113  114       11           12  36.45  0.967    NaN   \n",
      "\n",
      "                                            detailHref  \n",
      "111  /paper/extracting-motion-and-appearance-via-inter  \n",
      "112   /paper/film-frame-interpolation-for-large-motion  \n",
      "113  /paper/many-to-many-splatting-for-efficient-video  , 13:       id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "114  115        2           13  34.67  0.907    NaN  \\\n",
      "115  116        8           13  33.78  0.906    NaN   \n",
      "\n",
      "                                            detailHref  \n",
      "114  /paper/extracting-motion-and-appearance-via-inter  \n",
      "115   /paper/film-frame-interpolation-for-large-motion  , 14:       id  modelId  benchmarkId    psnr  ssim  lpips   \n",
      "116  117       33           14  28.287   NaN    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "116  /paper/spatio-temporal-multi-flow-network-for-...  , 15:       id  modelId  benchmarkId    psnr  ssim  lpips   \n",
      "117  118       33           15  29.175   NaN    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "117  /paper/spatio-temporal-multi-flow-network-for-...  , 16:       id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "118  119       11           16  29.03  0.959    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "118  /paper/many-to-many-splatting-for-efficient-video  , 17:       id  modelId  benchmarkId   psnr   ssim  lpips   \n",
      "119  120       11           17  33.93  0.945    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "119  /paper/many-to-many-splatting-for-efficient-video  , 18:       id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "120  121       10           18  36.24  0.9839    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "120  /paper/learning-cross-video-neural-representat...  , 19:       id  modelId  benchmarkId   psnr    ssim  lpips   \n",
      "121  122       10           19  30.94  0.9389    NaN  \\\n",
      "\n",
      "                                            detailHref  \n",
      "121  /paper/learning-cross-video-neural-representat...  }\n"
     ]
    }
   ],
   "source": [
    "benchmark_dict = {}\n",
    "for bench_id in df_model2benchmarks.benchmarkId.unique():\n",
    "    #if model_id not in benchmark_dict:\n",
    "    benchmark_dict[bench_id] = df_model2benchmarks[df_model2benchmarks['benchmarkId'] == bench_id]\n",
    "print(benchmark_dict)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lordo\\AppData\\Local\\Temp\\ipykernel_15884\\356053758.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_dict[benchmark]['score'] = score\n",
      "C:\\Users\\lordo\\AppData\\Local\\Temp\\ipykernel_15884\\356053758.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_dict[benchmark]['score'] = score\n",
      "C:\\Users\\lordo\\AppData\\Local\\Temp\\ipykernel_15884\\356053758.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_dict[benchmark]['score'] = score\n",
      "C:\\Users\\lordo\\AppData\\Local\\Temp\\ipykernel_15884\\356053758.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_dict[benchmark]['score'] = score\n",
      "C:\\Users\\lordo\\AppData\\Local\\Temp\\ipykernel_15884\\356053758.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  benchmark_dict[benchmark]['score'] = score\n"
     ]
    }
   ],
   "source": [
    "for benchmark in benchmark_dict:\n",
    "    if benchmark < 6:\n",
    "        delka = len(benchmark_dict[benchmark])\n",
    "        score = []\n",
    "        for i in range(delka):\n",
    "            score.append(1 - i/delka)\n",
    "        benchmark_dict[benchmark]['score'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "    id  modelId  benchmarkId   psnr    ssim  lpips   \n0    1        1            1  36.76  0.9800    NaN  \\\n1    2        2            1  36.64  0.9819    NaN   \n2    3        3            1  36.42  0.9815    NaN   \n3    4        4            1  36.20  0.9808    NaN   \n4    5        5            1  36.19  0.9810    NaN   \n..  ..      ...          ...    ...     ...    ...   \n78  79       21            5    NaN     NaN    NaN   \n79  80       20            5    NaN     NaN    NaN   \n80  81        1            5  38.83     NaN    NaN   \n81  82        8            5  37.52  0.9660    NaN   \n82  83       13            5  37.14  0.9660  0.007   \n\n                                           detailHref     score  \n0   /paper/exploring-motion-ambiguity-and-alignmen...  1.000000  \n1   /paper/extracting-motion-and-appearance-via-inter  0.954545  \n2   /paper/a-unified-pyramid-recurrent-network-for...  0.909091  \n3   /paper/ifrnet-intermediate-feature-refine-network  0.863636  \n4   /paper/enhanced-bi-directional-motion-estimati...  0.818182  \n..                                                ...       ...  \n78   /paper/video-enhancement-with-task-oriented-flow  0.454545  \n79      /paper/video-frame-interpolation-via-adaptive  0.363636  \n80  /paper/exploring-motion-ambiguity-and-alignmen...  0.272727  \n81   /paper/film-frame-interpolation-for-large-motion  0.181818  \n82  /paper/cdfi-compression-driven-network-design-for  0.090909  \n\n[83 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>modelId</th>\n      <th>benchmarkId</th>\n      <th>psnr</th>\n      <th>ssim</th>\n      <th>lpips</th>\n      <th>detailHref</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>36.76</td>\n      <td>0.9800</td>\n      <td>NaN</td>\n      <td>/paper/exploring-motion-ambiguity-and-alignmen...</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>36.64</td>\n      <td>0.9819</td>\n      <td>NaN</td>\n      <td>/paper/extracting-motion-and-appearance-via-inter</td>\n      <td>0.954545</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>36.42</td>\n      <td>0.9815</td>\n      <td>NaN</td>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>36.20</td>\n      <td>0.9808</td>\n      <td>NaN</td>\n      <td>/paper/ifrnet-intermediate-feature-refine-network</td>\n      <td>0.863636</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>36.19</td>\n      <td>0.9810</td>\n      <td>NaN</td>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>0.818182</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>79</td>\n      <td>21</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/paper/video-enhancement-with-task-oriented-flow</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>80</td>\n      <td>20</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/paper/video-frame-interpolation-via-adaptive</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>81</td>\n      <td>1</td>\n      <td>5</td>\n      <td>38.83</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>/paper/exploring-motion-ambiguity-and-alignmen...</td>\n      <td>0.272727</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>82</td>\n      <td>8</td>\n      <td>5</td>\n      <td>37.52</td>\n      <td>0.9660</td>\n      <td>NaN</td>\n      <td>/paper/film-frame-interpolation-for-large-motion</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>83</td>\n      <td>13</td>\n      <td>5</td>\n      <td>37.14</td>\n      <td>0.9660</td>\n      <td>0.007</td>\n      <td>/paper/cdfi-compression-driven-network-design-for</td>\n      <td>0.090909</td>\n    </tr>\n  </tbody>\n</table>\n<p>83 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for benchmark in benchmark_dict:\n",
    "    if benchmark < 6:\n",
    "        if df_final.empty:\n",
    "            df_final = benchmark_dict[benchmark]\n",
    "        else:\n",
    "            df_final = pd.concat([df_final, benchmark_dict[benchmark]])\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "          id  benchmarkId     psnr    ssim  lpips   \nmodelId                                             \n2        128           10  133.470  2.9050  0.022  \\\n4        121            9   71.620  1.9506  0.000   \n3         72            6  101.620  2.9025  0.025   \n6        149           10  129.710  3.7486  0.039   \n7        127            9  109.910  2.8930  0.000   \n9        116            8  102.870  2.8675  0.000   \n1        125            9  111.020  1.9590  0.000   \n5        115            8  101.060  2.8530  0.000   \n8        171           11  137.010  3.8160  0.033   \n11       123            8  101.380  2.8600  0.000   \n17       234           14  131.190  2.7509  0.000   \n10        90            6   99.100  2.8694  0.029   \n15       181           11   93.500  2.8303  0.071   \n13       182           11  134.510  1.8740  0.083   \n16        78            5   65.460  1.8468  0.000   \n22        23            2   27.150  0.9140  0.039   \n23        24            2   27.350  0.9130  0.061   \n18        95            6   34.650  0.0000  0.000   \n24        25            2   26.690  0.9040  0.068   \n35        61            4   30.680  0.9086  0.000   \n25        28            2   28.770  0.9310  0.024   \n12        67            4   70.150  1.9139  0.000   \n26        29            2   28.560  0.9280  0.028   \n27        30            2   28.340  0.9170  0.044   \n36        64            4   30.120  0.8700  0.000   \n39        78            5    0.000  0.0000  0.000   \n29       105            6   50.800  1.6750  0.058   \n21       101            6   33.730  0.0000  0.000   \n37        66            4   28.860  0.8580  0.000   \n20       101            6   33.800  0.0000  0.000   \n30       107            6   49.640  1.6850  0.060   \n14        14            1   35.070  0.9760  0.000   \n38        67            4   27.520  0.8210  0.000   \n28        34            2   27.860  0.9210  0.049   \n31       110            6   47.280  1.6450  0.070   \n19        20            1   34.400  0.0000  0.000   \n32       112            6   47.070  1.6180  0.692   \n33        56            3   33.384  0.0000  0.000   \n34        57            3    0.000  0.0000  0.000   \n\n                                                detailHref     score  \nmodelId                                                               \n2        /paper/extracting-motion-and-appearance-via-in...  3.721212  \n4        /paper/ifrnet-intermediate-feature-refine-netw...  2.687166  \n3        /paper/a-unified-pyramid-recurrent-network-for...  2.628045  \n6        /paper/asymmetric-bilateral-motion-estimation-...  2.530897  \n7        /paper/softmax-splatting-for-video-frame/paper...  2.342246  \n9        /paper/neighbor-correspondence-matching-for-fl...  2.165775  \n1        /paper/exploring-motion-ambiguity-and-alignmen...  2.155080  \n5        /paper/enhanced-bi-directional-motion-estimati...  2.116221  \n8        /paper/film-frame-interpolation-for-large-moti...  1.889780  \n11       /paper/many-to-many-splatting-for-efficient-vi...  1.765062  \n17       /paper/depth-aware-video-frame-interpolation/p...  1.704991  \n10       /paper/learning-cross-video-neural-representat...  1.679144  \n15       /paper/bmbc-bilateral-motion-estimation-with/p...  1.642602  \n13       /paper/cdfi-compression-driven-network-design-...  1.290553  \n16       /paper/enhanced-correlation-matching-based-vid...  1.051515  \n22       /paper/rife-real-time-intermediate-flow-estima...  1.000000  \n23           /paper/xvfi-extreme-video-frame-interpolation  0.944444  \n18                                                nullnull  0.909091  \n24           /paper/super-slomo-high-quality-estimation-of  0.888889  \n35       /paper/a-unified-pyramid-recurrent-network-for...  0.800000  \n25       /paper/enhanced-bi-directional-motion-estimati...  0.722222  \n12       /paper/video-frame-interpolation-via-residue/p...  0.676471  \n26       /paper/enhanced-bi-directional-motion-estimati...  0.666667  \n27       /paper/video-frame-interpolation-with-transformer  0.611111  \n36           /paper/xvfi-extreme-video-frame-interpolation  0.600000  \n39          /paper/memc-net-motion-estimation-and-motion-1  0.545455  \n29       /paper/learning-spatial-transform-for-video-fr...  0.544444  \n21       /paper/video-enhancement-with-task-oriented-fl...  0.500000  \n37           /paper/xvfi-extreme-video-frame-interpolation  0.466667  \n20       /paper/video-frame-interpolation-via-adaptive/...  0.454545  \n30       /paper/featureflow-robust-video-interpolation-...  0.422222  \n14           /paper/xvfi-extreme-video-frame-interpolation  0.409091  \n38            /paper/depth-aware-video-frame-interpolation  0.400000  \n28           /paper/xvfi-extreme-video-frame-interpolation  0.388889  \n31       /paper/featureflow-robust-video-interpolation-...  0.244444  \n19            /paper/memc-net-motion-estimation-and-motion  0.136364  \n32       /paper/learning-spatial-transform-for-video-fr...  0.122222  \n33       /paper/spatio-temporal-multi-flow-network-for-...  0.117647  \n34       /paper/unsupervised-video-interpolation-using-...  0.058824  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>benchmarkId</th>\n      <th>psnr</th>\n      <th>ssim</th>\n      <th>lpips</th>\n      <th>detailHref</th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>modelId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>128</td>\n      <td>10</td>\n      <td>133.470</td>\n      <td>2.9050</td>\n      <td>0.022</td>\n      <td>/paper/extracting-motion-and-appearance-via-in...</td>\n      <td>3.721212</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>121</td>\n      <td>9</td>\n      <td>71.620</td>\n      <td>1.9506</td>\n      <td>0.000</td>\n      <td>/paper/ifrnet-intermediate-feature-refine-netw...</td>\n      <td>2.687166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>72</td>\n      <td>6</td>\n      <td>101.620</td>\n      <td>2.9025</td>\n      <td>0.025</td>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n      <td>2.628045</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>149</td>\n      <td>10</td>\n      <td>129.710</td>\n      <td>3.7486</td>\n      <td>0.039</td>\n      <td>/paper/asymmetric-bilateral-motion-estimation-...</td>\n      <td>2.530897</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>127</td>\n      <td>9</td>\n      <td>109.910</td>\n      <td>2.8930</td>\n      <td>0.000</td>\n      <td>/paper/softmax-splatting-for-video-frame/paper...</td>\n      <td>2.342246</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>116</td>\n      <td>8</td>\n      <td>102.870</td>\n      <td>2.8675</td>\n      <td>0.000</td>\n      <td>/paper/neighbor-correspondence-matching-for-fl...</td>\n      <td>2.165775</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>125</td>\n      <td>9</td>\n      <td>111.020</td>\n      <td>1.9590</td>\n      <td>0.000</td>\n      <td>/paper/exploring-motion-ambiguity-and-alignmen...</td>\n      <td>2.155080</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>115</td>\n      <td>8</td>\n      <td>101.060</td>\n      <td>2.8530</td>\n      <td>0.000</td>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>2.116221</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>171</td>\n      <td>11</td>\n      <td>137.010</td>\n      <td>3.8160</td>\n      <td>0.033</td>\n      <td>/paper/film-frame-interpolation-for-large-moti...</td>\n      <td>1.889780</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>123</td>\n      <td>8</td>\n      <td>101.380</td>\n      <td>2.8600</td>\n      <td>0.000</td>\n      <td>/paper/many-to-many-splatting-for-efficient-vi...</td>\n      <td>1.765062</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>234</td>\n      <td>14</td>\n      <td>131.190</td>\n      <td>2.7509</td>\n      <td>0.000</td>\n      <td>/paper/depth-aware-video-frame-interpolation/p...</td>\n      <td>1.704991</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>90</td>\n      <td>6</td>\n      <td>99.100</td>\n      <td>2.8694</td>\n      <td>0.029</td>\n      <td>/paper/learning-cross-video-neural-representat...</td>\n      <td>1.679144</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>181</td>\n      <td>11</td>\n      <td>93.500</td>\n      <td>2.8303</td>\n      <td>0.071</td>\n      <td>/paper/bmbc-bilateral-motion-estimation-with/p...</td>\n      <td>1.642602</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>182</td>\n      <td>11</td>\n      <td>134.510</td>\n      <td>1.8740</td>\n      <td>0.083</td>\n      <td>/paper/cdfi-compression-driven-network-design-...</td>\n      <td>1.290553</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78</td>\n      <td>5</td>\n      <td>65.460</td>\n      <td>1.8468</td>\n      <td>0.000</td>\n      <td>/paper/enhanced-correlation-matching-based-vid...</td>\n      <td>1.051515</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>2</td>\n      <td>27.150</td>\n      <td>0.9140</td>\n      <td>0.039</td>\n      <td>/paper/rife-real-time-intermediate-flow-estima...</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>2</td>\n      <td>27.350</td>\n      <td>0.9130</td>\n      <td>0.061</td>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>95</td>\n      <td>6</td>\n      <td>34.650</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>nullnull</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>2</td>\n      <td>26.690</td>\n      <td>0.9040</td>\n      <td>0.068</td>\n      <td>/paper/super-slomo-high-quality-estimation-of</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>61</td>\n      <td>4</td>\n      <td>30.680</td>\n      <td>0.9086</td>\n      <td>0.000</td>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>28</td>\n      <td>2</td>\n      <td>28.770</td>\n      <td>0.9310</td>\n      <td>0.024</td>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>0.722222</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>67</td>\n      <td>4</td>\n      <td>70.150</td>\n      <td>1.9139</td>\n      <td>0.000</td>\n      <td>/paper/video-frame-interpolation-via-residue/p...</td>\n      <td>0.676471</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>29</td>\n      <td>2</td>\n      <td>28.560</td>\n      <td>0.9280</td>\n      <td>0.028</td>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>30</td>\n      <td>2</td>\n      <td>28.340</td>\n      <td>0.9170</td>\n      <td>0.044</td>\n      <td>/paper/video-frame-interpolation-with-transformer</td>\n      <td>0.611111</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>64</td>\n      <td>4</td>\n      <td>30.120</td>\n      <td>0.8700</td>\n      <td>0.000</td>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>78</td>\n      <td>5</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/memc-net-motion-estimation-and-motion-1</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>105</td>\n      <td>6</td>\n      <td>50.800</td>\n      <td>1.6750</td>\n      <td>0.058</td>\n      <td>/paper/learning-spatial-transform-for-video-fr...</td>\n      <td>0.544444</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>101</td>\n      <td>6</td>\n      <td>33.730</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/video-enhancement-with-task-oriented-fl...</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>66</td>\n      <td>4</td>\n      <td>28.860</td>\n      <td>0.8580</td>\n      <td>0.000</td>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.466667</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>101</td>\n      <td>6</td>\n      <td>33.800</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/video-frame-interpolation-via-adaptive/...</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>107</td>\n      <td>6</td>\n      <td>49.640</td>\n      <td>1.6850</td>\n      <td>0.060</td>\n      <td>/paper/featureflow-robust-video-interpolation-...</td>\n      <td>0.422222</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>1</td>\n      <td>35.070</td>\n      <td>0.9760</td>\n      <td>0.000</td>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.409091</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>67</td>\n      <td>4</td>\n      <td>27.520</td>\n      <td>0.8210</td>\n      <td>0.000</td>\n      <td>/paper/depth-aware-video-frame-interpolation</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>34</td>\n      <td>2</td>\n      <td>27.860</td>\n      <td>0.9210</td>\n      <td>0.049</td>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.388889</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>110</td>\n      <td>6</td>\n      <td>47.280</td>\n      <td>1.6450</td>\n      <td>0.070</td>\n      <td>/paper/featureflow-robust-video-interpolation-...</td>\n      <td>0.244444</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>1</td>\n      <td>34.400</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/memc-net-motion-estimation-and-motion</td>\n      <td>0.136364</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>112</td>\n      <td>6</td>\n      <td>47.070</td>\n      <td>1.6180</td>\n      <td>0.692</td>\n      <td>/paper/learning-spatial-transform-for-video-fr...</td>\n      <td>0.122222</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>56</td>\n      <td>3</td>\n      <td>33.384</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/spatio-temporal-multi-flow-network-for-...</td>\n      <td>0.117647</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>57</td>\n      <td>3</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>/paper/unsupervised-video-interpolation-using-...</td>\n      <td>0.058824</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df_final.groupby('modelId').sum()\n",
    "df_final.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 3, 6, 7, 9, 1, 5, 8, 11, 17, 10, 15, 13, 16, 22, 23, 18, 24, 35, 25, 12, 26, 27, 36, 39, 29, 21, 37, 20, 30, 14, 38, 28, 31, 19, 32, 33, 34]\n",
      "[3.721212121212121, 2.6871657754010694, 2.6280451574569224, 2.5308972073677953, 2.342245989304813, 2.165775401069519, 2.1550802139037435, 2.1162210338680927, 1.8897801544860369, 1.7650623885918004, 1.7049910873440286, 1.679144385026738, 1.642602495543672, 1.2905525846702317, 1.0515151515151517, 1.0, 0.9444444444444444, 0.9090909090909091, 0.8888888888888888, 0.8, 0.7222222222222222, 0.6764705882352942, 0.6666666666666667, 0.6111111111111112, 0.6, 0.5454545454545454, 0.5444444444444445, 0.5, 0.4666666666666667, 0.4545454545454546, 0.42222222222222217, 0.40909090909090906, 0.4, 0.38888888888888884, 0.24444444444444446, 0.13636363636363635, 0.12222222222222223, 0.11764705882352944, 0.05882352941176472]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "model_ids = []\n",
    "scores = []\n",
    "for row in df_final.iterrows():\n",
    "    model_ids.append(row[0])\n",
    "    scores.append(list(row[1])[-1])\n",
    "print(model_ids)\n",
    "print(scores)\n",
    "idx_list = [i for i in range(len(model_ids))]\n",
    "print(idx_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: >"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=0.5)\n",
    "sns.barplot(x=model_ids, y=scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                detailHref     score\nmodelId                                                             \n2        /paper/extracting-motion-and-appearance-via-in...  3.721212\n4        /paper/ifrnet-intermediate-feature-refine-netw...  2.687166\n3        /paper/a-unified-pyramid-recurrent-network-for...  2.628045\n6        /paper/asymmetric-bilateral-motion-estimation-...  2.530897\n7        /paper/softmax-splatting-for-video-frame/paper...  2.342246\n9        /paper/neighbor-correspondence-matching-for-fl...  2.165775\n1        /paper/exploring-motion-ambiguity-and-alignmen...  2.155080\n5        /paper/enhanced-bi-directional-motion-estimati...  2.116221\n8        /paper/film-frame-interpolation-for-large-moti...  1.889780\n11       /paper/many-to-many-splatting-for-efficient-vi...  1.765062\n17       /paper/depth-aware-video-frame-interpolation/p...  1.704991\n10       /paper/learning-cross-video-neural-representat...  1.679144\n15       /paper/bmbc-bilateral-motion-estimation-with/p...  1.642602\n13       /paper/cdfi-compression-driven-network-design-...  1.290553\n16       /paper/enhanced-correlation-matching-based-vid...  1.051515\n22       /paper/rife-real-time-intermediate-flow-estima...  1.000000\n23           /paper/xvfi-extreme-video-frame-interpolation  0.944444\n18                                                nullnull  0.909091\n24           /paper/super-slomo-high-quality-estimation-of  0.888889\n35       /paper/a-unified-pyramid-recurrent-network-for...  0.800000\n25       /paper/enhanced-bi-directional-motion-estimati...  0.722222\n12       /paper/video-frame-interpolation-via-residue/p...  0.676471\n26       /paper/enhanced-bi-directional-motion-estimati...  0.666667\n27       /paper/video-frame-interpolation-with-transformer  0.611111\n36           /paper/xvfi-extreme-video-frame-interpolation  0.600000\n39          /paper/memc-net-motion-estimation-and-motion-1  0.545455\n29       /paper/learning-spatial-transform-for-video-fr...  0.544444\n21       /paper/video-enhancement-with-task-oriented-fl...  0.500000\n37           /paper/xvfi-extreme-video-frame-interpolation  0.466667\n20       /paper/video-frame-interpolation-via-adaptive/...  0.454545\n30       /paper/featureflow-robust-video-interpolation-...  0.422222\n14           /paper/xvfi-extreme-video-frame-interpolation  0.409091\n38            /paper/depth-aware-video-frame-interpolation  0.400000\n28           /paper/xvfi-extreme-video-frame-interpolation  0.388889\n31       /paper/featureflow-robust-video-interpolation-...  0.244444\n19            /paper/memc-net-motion-estimation-and-motion  0.136364\n32       /paper/learning-spatial-transform-for-video-fr...  0.122222\n33       /paper/spatio-temporal-multi-flow-network-for-...  0.117647\n34       /paper/unsupervised-video-interpolation-using-...  0.058824",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>detailHref</th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>modelId</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>/paper/extracting-motion-and-appearance-via-in...</td>\n      <td>3.721212</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/paper/ifrnet-intermediate-feature-refine-netw...</td>\n      <td>2.687166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n      <td>2.628045</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/paper/asymmetric-bilateral-motion-estimation-...</td>\n      <td>2.530897</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/paper/softmax-splatting-for-video-frame/paper...</td>\n      <td>2.342246</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/paper/neighbor-correspondence-matching-for-fl...</td>\n      <td>2.165775</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/paper/exploring-motion-ambiguity-and-alignmen...</td>\n      <td>2.155080</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>2.116221</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/paper/film-frame-interpolation-for-large-moti...</td>\n      <td>1.889780</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>/paper/many-to-many-splatting-for-efficient-vi...</td>\n      <td>1.765062</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>/paper/depth-aware-video-frame-interpolation/p...</td>\n      <td>1.704991</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>/paper/learning-cross-video-neural-representat...</td>\n      <td>1.679144</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>/paper/bmbc-bilateral-motion-estimation-with/p...</td>\n      <td>1.642602</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>/paper/cdfi-compression-driven-network-design-...</td>\n      <td>1.290553</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>/paper/enhanced-correlation-matching-based-vid...</td>\n      <td>1.051515</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>/paper/rife-real-time-intermediate-flow-estima...</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>nullnull</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>/paper/super-slomo-high-quality-estimation-of</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>/paper/a-unified-pyramid-recurrent-network-for...</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>0.722222</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>/paper/video-frame-interpolation-via-residue/p...</td>\n      <td>0.676471</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>/paper/enhanced-bi-directional-motion-estimati...</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>/paper/video-frame-interpolation-with-transformer</td>\n      <td>0.611111</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>/paper/memc-net-motion-estimation-and-motion-1</td>\n      <td>0.545455</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>/paper/learning-spatial-transform-for-video-fr...</td>\n      <td>0.544444</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>/paper/video-enhancement-with-task-oriented-fl...</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.466667</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>/paper/video-frame-interpolation-via-adaptive/...</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>/paper/featureflow-robust-video-interpolation-...</td>\n      <td>0.422222</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.409091</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>/paper/depth-aware-video-frame-interpolation</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>/paper/xvfi-extreme-video-frame-interpolation</td>\n      <td>0.388889</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>/paper/featureflow-robust-video-interpolation-...</td>\n      <td>0.244444</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>/paper/memc-net-motion-estimation-and-motion</td>\n      <td>0.136364</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>/paper/learning-spatial-transform-for-video-fr...</td>\n      <td>0.122222</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>/paper/spatio-temporal-multi-flow-network-for-...</td>\n      <td>0.117647</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>/paper/unsupervised-video-interpolation-using-...</td>\n      <td>0.058824</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df_final)\n",
    "df_fin = df_final.drop(['benchmarkId', 'psnr', \"lpips\", \"ssim\", \"id\"], axis=1)\n",
    "df_fin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
